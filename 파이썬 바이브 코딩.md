

# Streamlit ì„¤ì¹˜ í›„ ì‹¤í–‰
## ì„¤ì¹˜
```
pip install streamlit
```
## ì‹¤í–‰ ì•ˆë¨
```shell
streamlit hello
```
ì´ê±¸ë¡œ ì‹¤í–‰í•˜ë¼ëŠ”ê±´ë° ì•ˆë¼

```shell
PS C:\dev\workspace\python-vibe-coding> streamlit hello
streamlit : 'streamlit' ìš©ì–´ê°€ cmdlet, í•¨ìˆ˜, ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ ë˜ëŠ” ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” í”„ë¡œê·¸ë¨ ì´ë¦„ìœ¼ë¡œ ì¸ì‹ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ë¦„ì´ ì •í™•í•œì§€ í™•ì¸í•˜ê³  ê²½ë¡œê°€ í¬í•¨ëœ ê²½ìš° ê²½ë¡œê°€ ì˜¬ë°”ë¥¸ì§€ ê²€ì¦í•œ ë‹¤ìŒ ë‹¤
ì‹œ ì‹œë„í•˜ì‹­ì‹œì˜¤.
ìœ„ì¹˜ ì¤„:1 ë¬¸ì:1
+ streamlit hello
+ ~~~~~~~~~
    + CategoryInfo          : ObjectNotFound: (streamlit:String*) [], CommandNotFoundException
    + FullyQualifiedErrorId : CommandNotFoundException

PS C:\dev\workspace\python-vibe-coding> streamlit hello
streamlit : 'streamlit' ìš©ì–´ê°€ cmdlet, í•¨ìˆ˜, ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ ë˜ëŠ” ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” í”„ë¡œê·¸ë¨ ì´ë¦„ìœ¼ë¡œ ì¸ì‹ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ë¦„ì´ ì •í™•í•œì§€ í™•ì¸í•˜ê³  ê²½ë¡œê°€ í¬í•¨ëœ ê²½ìš° ê²½ë¡œê°€ ì˜¬ë°”ë¥¸ì§€* ê²€ì¦í•œ ë‹¤ìŒ ë‹¤
ì‹œ ì‹œë„í•˜ì‹­ì‹œì˜¤.
ìœ„ì¹˜ ì¤„:1 ë¬¸ì:1
+ streamlit hello
+ ~~~~~~~~~
    + CategoryInfo          : ObjectNotFound: (streamlit:String) [], CommandNotFoundException
    + FullyQualifiedErrorId : CommandNotFoundException
```

## ì‹¤í–‰ë¨
```shell
python -m streamlit hello
```
ì´ë ‡ê²Œ ì‹¤í–‰í•˜ë‹ˆê¹ ë¨

> ì´ê±´ ê·¸ëƒ¥ í•´ë‹¹ ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ê¸°ë³¸ì ìœ¼ë¡œ ì œê³µí•˜ëŠ” í…œí”Œë¦¿ì„.

# íŒŒì¼ ë§Œë“¤ê³  ì‹¤í–‰
```python
import streamlit as st

st.title("Hello Streamlit! ğŸš€")
st.write("ì´ê²ƒì€ Streamlitìœ¼ë¡œ ë§Œë“  ê°„ë‹¨í•œ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì…ë‹ˆë‹¤.")
st.write("ë°”ë¡œ ì‹¤ì‹œê°„ ì ìš© ë¼ìš”?") # ëœë‹¤ ê°œì‹ ê¸°í•¨
```

```shell
python -m streamlit run hello_streamlit.py
```

> ì§ì ‘ ë§Œë“¤ë©´ ì´ëŸ° ì‹ìœ¼ë¡œ ì‘ì„±í•˜ê³  ì‹¤í–‰í•˜ëŠ”ê±°ì„

# ë‚ ë¨¹
ì´ë¯¸ ì‚¬ëŒë“¤ì´ ë§Œë“¤ì–´ ë†“ì€ê±¸ ì‚¬ìš©í•˜ì
[ê¹ƒí—ˆë¸Œ ë ˆí¬ì§€í† ë¦¬](https://github.com/shashankdeshpande/langchain-chatbot?tab=readme-ov-file)

# ì¶”ê°€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
## ì‚½ì§ˆ
![[Pasted image 20251126191001.png]]
ì‚¬ìš©í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì „ë¶€ ë‹¤ìš´

```shell
pip install streamlit openai langchain langchain-openai langchain-community fastembed
```
- openai : 
- langchain : 
- langchain-openai : 
- langchain-community : 
- fastembed : 

![[Pasted image 20251126191647.png]]

ì´ê²ƒë„ ì„¤ì¹˜
```shell
pip install pypdf docarray pydantic==1.10.9
```
- pypdf : 
- docarray : 
- pydantic : 

### ì¶”ê°€ ì—ëŸ¬
```shell
ImportError: module ''langchain_core._api'.'deprecation'' not found (No module named 'pydantic.v1')

File "C:\dev\workspace\python-vibe-coding\langchain-chatbot\pages\4_ğŸ“„_chat_with_your_documents.py", line 2, in <module>
    import utils
File "C:\dev\workspace\python-vibe-coding\langchain-chatbot\utils.py", line 6, in <module>
    from langchain_openai import ChatOpenAI
File "C:\Users\young\AppData\Roaming\Python\Python313\site-packages\langchain_openai\__init__.py", line 3, in <module>
    from langchain_openai.chat_models import AzureChatOpenAI, ChatOpenAI
File "C:\Users\young\AppData\Roaming\Python\Python313\site-packages\langchain_openai\chat_models\__init__.py", line 3, in <module>
    from langchain_openai.chat_models.azure import AzureChatOpenAI
File "C:\Users\young\AppData\Roaming\Python\Python313\site-packages\langchain_openai\chat_models\azure.py", line 11, in <module>
    from langchain_core.language_models import LanguageModelInput
File "C:\Users\young\AppData\Roaming\Python\Python313\site-packages\langchain_core\__init__.py", line 11, in <module>
    from langchain_core._api import (
    ...<2 lines>...
    )
File "C:\Users\young\AppData\Roaming\Python\Python313\site-packages\langchain_core\_api\__init__.py", line 62, in __getattr__
    result = import_attr(attr_name, module_name, __spec__.parent)
File "C:\Users\young\AppData\Roaming\Python\Python313\site-packages\langchain_core\_import_utils.py", line 38, in import_attr
    raise ImportError(msg) from None
```

### ë²„ì „ ë¶ˆì¼ì¹˜ë¡œ ì¸í•œ update
```shell
pip install --upgrade pydantic langchain langchain-community langchain-core langchain-openai
```

### ì¶©ëŒë‚œê±° ë°€ê³  ë‹¤ì‹œ ë‹¤ìš´
```shell
pip uninstall -y langchain langchain-community langchain-core langchain-openai langchain-text-splitters langchain-classic
```

```shell
python -m pip install langchain==0.1.20 langchain-community==0.0.38 langchain-openai==0.1.6 langchain-core==0.1.52 pypdf docarray pydantic==1.10.9
```

# ë¬¸ì„œ ê¸°ë°˜ ì„¤ì •ì •
## ë¬¸ì„œ í™•ì¸
```
langchain==0.3.13
langchainhub==0.1.21
langchain_community==0.3.13
langchain_core==0.3.28
langchain_openai==0.2.14
langchain_text_splitters==0.3.4
openai==1.58.1
Requests==2.32.3
SQLAlchemy==2.0.36
streamlit==1.41.1
validators==0.34.0
fastembed==0.4.2
pypdf==5.1.0
duckduckgo_search==7.0.1
pydantic==2.10.6
```
ì‚¬ì‹¤ ì•ˆë‚´ë˜ëŠ” ë²„ì „ ì •ë³´ê°€ ìˆì—ˆìŒ.

ê·¼ë° ì´ê²ƒë„ ì‹¤íŒ¨
```
PS C:\dev\workspace\python-vibe-coding\langchain-chatbot> python -m pip install -r requirements.txt                                  
Defaulting to user installation because normal site-packages is not writeable
Collecting langchain==0.3.13 (from -r requirements.txt (line 1))
  Downloading langchain-0.3.13-py3-none-any.whl.metadata (7.1 kB)
Collecting langchainhub==0.1.21 (from -r requirements.txt (line 2))
  Downloading langchainhub-0.1.21-py3-none-any.whl.metadata (659 bytes)
Collecting langchain_community==0.3.13 (from -r requirements.txt (line 3))
  Downloading langchain_community-0.3.13-py3-none-any.whl.metadata (2.9 kB)
Collecting langchain_core==0.3.28 (from -r requirements.txt (line 4))
  Downloading langchain_core-0.3.28-py3-none-any.whl.metadata (6.3 kB)
Collecting langchain_openai==0.2.14 (from -r requirements.txt (line 5))
  Downloading langchain_openai-0.2.14-py3-none-any.whl.metadata (2.7 kB)
Collecting langchain_text_splitters==0.3.4 (from -r requirements.txt (line 6))
  Downloading langchain_text_splitters-0.3.4-py3-none-any.whl.metadata (2.3 kB)
Collecting openai==1.58.1 (from -r requirements.txt (line 7))
  Downloading openai-1.58.1-py3-none-any.whl.metadata (27 kB)
Collecting Requests==2.32.3 (from -r requirements.txt (line 8))
  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)
Collecting SQLAlchemy==2.0.36 (from -r requirements.txt (line 9))
  Downloading SQLAlchemy-2.0.36-cp313-cp313-win_amd64.whl.metadata (9.9 kB)
Collecting streamlit==1.41.1 (from -r requirements.txt (line 10))
  Downloading streamlit-1.41.1-py2.py3-none-any.whl.metadata (8.5 kB)
Collecting validators==0.34.0 (from -r requirements.txt (line 11))
  Downloading validators-0.34.0-py3-none-any.whl.metadata (3.8 kB)
ERROR: Ignored the following versions that require a different python version: 0.0.1 Requires-Python >=3.8.0,<3.12; 0.0.1a1 Requires-Python >=3.8.0,<3.12; 0.0.1a2 Requires-Python >=3.8.0,<3.12; 0.0.1a3 Requires-Python >=3.8.0,<3.12; 0.0.2 Requires-Python >=3.8.0,<3.12; 0.0.2a0 Requires-Python >=3.8.0,<3.12; 0.0.3 Requires-Python >=3.8.0,<3.12; 0.0.3a1 Requires-Python >=3.8.0,<3.12; 0.0.4 Requires-Python >=3.8.0,<3.12; 0.0.5 Requires-Python >=3.8.0,<3.12; 0.0.5a1 Requires-Python >=3.8.0,<3.12; 0.0.5a2 Requires-Python >=3.8.0,<3.12; 0.1.0 Requires-Python >=3.8.0,<3.12; 0.1.1 Requires-Python >=3.8.0,<3.12; 0.1.2 Requires-Python >=3.8.0,<3.12; 0.1.3 Requires-Python >=3.8.0,<3.12; 0.2.0 Requires-Python >=3.8.0,<3.13; 0.2.1 Requires-Python >=3.8.0,<3.13; 0.2.2 Requires-Python >=3.8.0,<3.13; 0.2.3 Requires-Python >=3.8.0,<3.13; 0.2.4 Requires-Python >=3.8.0,<3.13; 0.2.5 Requires-Python >=3.8.0,<3.13; 0.2.6 Requires-Python >=3.8.0,<3.13; 0.2.7 Requires-Python >=3.8.0,<3.13; 0.3.0 Requires-Python >=3.8.0,<3.13; 0.3.1 Requires-Python >=3.8.0,<3.13; 0.3.2 Requires-Python >=3.8.0,<3.13; 0.3.3 Requires-Python >=3.8.0,<3.13; 0.3.4 Requires-Python >=3.8.0,<3.13; 0.3.5 Requires-Python >=3.8.0,<3.13; 0.3.6 Requires-Python >=3.8.0,<3.13; 0.4.0 Requires-Python >=3.8.0,<3.13; 0.4.1 Requires-Python >=3.8.0,<3.13; 0.4.2 Requires-Python >=3.8.0,<3.13; 0.55.2 Requires-Python <3.5
ERROR: Could not find a version that satisfies the requirement fastembed==0.4.2 (from versions: 0.5.0, 0.5.1, 0.6.0, 0.6.1, 0.7.0, 0.7.1, 0.7.2, 0.7.3)

[notice] A new release of pip is available: 25.2 -> 25.3
[notice] To update, run: python.exe -m pip install --upgrade pip
ERROR: No matching distribution found for fastembed==0.4.2
```

## íŒŒì´ì¬ ë²„ì „ ì´ìŠˆ
íŒŒì´ì¬ ë²„ì „ ë•Œë¬¸ì— ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ ë³€ê²½
```shell
langchain==0.3.13
langchainhub==0.1.21
langchain_community==0.3.13
langchain_core==0.3.28
langchain_openai==0.2.14
langchain_text_splitters==0.3.4
openai==1.58.1
Requests==2.32.3
SQLAlchemy==2.0.36
streamlit==1.41.1
validators==0.34.0
fastembed>=0.5.0 # ìˆ˜ì •
pypdf==5.1.0
duckduckgo_search==7.0.1
pydantic==2.10.6
```

ë²„ì „ ìˆ˜ì • í›„ ë°”ë¡œ ì‹¤í–‰
### ë¼ì´ë¸ŒëŸ¬ë¦¬ ì´ˆê¸°í™”
```shell
pip uninstall -y langchain langchain-community langchain-core langchain-openai langchain-text-splitters langchainhub openai streamlit fastembed pypdf pydantic docarray
```
### ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë‹¤ìš´ ë°›ê¸°
```shell
python -m pip install -r requirements.txt
```

## ì‹¤íŒ¨
ì—°ê²°ì€ ëœë“¯ í•˜ë‚˜ ì±„íŒ… ì…ë ¥ì‹œ ì—ëŸ¬ ë°œìƒìƒ 
```
ImportError:Â Could not import docarray python package. Please install it with `pip install docarray`.

Traceback:

`   File "C:\dev\workspace\python-vibe-coding\langchain-chatbot\pages\4_ğŸ“„_chat_with_your_documents.py", line 111, in <module>     obj.main()     ~~~~~~~~^^  File "C:\dev\workspace\python-vibe-coding\langchain-chatbot\utils.py", line 35, in execute     func(*args, **kwargs)     ~~~~^^^^^^^^^^^^^^^^^  File "C:\dev\workspace\python-vibe-coding\langchain-chatbot\pages\4_ğŸ“„_chat_with_your_documents.py", line 87, in main     qa_chain = self.setup_qa_chain(uploaded_files)  File "C:\Python313\Lib\contextlib.py", line 85, in inner     return func(*args, **kwds)  File "C:\dev\workspace\python-vibe-coding\langchain-chatbot\pages\4_ğŸ“„_chat_with_your_documents.py", line 50, in setup_qa_chain     vectordb = DocArrayInMemorySearch.from_documents(splits, self.embedding_model)  File "C:\Users\young\AppData\Roaming\Python\Python313\site-packages\langchain_core\vectorstores\base.py", line 852, in from_documents     return cls.from_texts(texts, embedding, metadatas=metadatas, **kwargs)            ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  File "C:\Users\young\AppData\Roaming\Python\Python313\site-packages\langchain_community\vectorstores\docarray\in_memory.py", line 67, in from_texts     store = cls.from_params(embedding, **kwargs)  File "C:\Users\young\AppData\Roaming\Python\Python313\site-packages\langchain_community\vectorstores\docarray\in_memory.py", line 40, in from_params     _check_docarray_import()     ~~~~~~~~~~~~~~~~~~~~~~^^  File "C:\Users\young\AppData\Roaming\Python\Python313\site-packages\langchain_community\vectorstores\docarray\base.py", line 29, in _check_docarray_import     raise ImportError(     ...<2 lines>...     )   `
```

![[Pasted image 20251126194627.png]]

## ì¶”ê°€ ì„¤ì¹˜
ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ë˜ê±°ì˜€ìŒ
```shell
python -m pip install docarray
```

### ë¬¸ì„œ ìˆ˜ì •
```text
langchain==0.3.13
langchainhub==0.1.21
langchain_community==0.3.13
langchain_core==0.3.28
langchain_openai==0.2.14
langchain_text_splitters==0.3.4
openai==1.58.1
Requests==2.32.3
SQLAlchemy==2.0.36
streamlit==1.41.1
validators==0.34.0
fastembed>=0.5.0 # ìˆ˜ì •
pypdf==5.1.0
duckduckgo_search==7.0.1
pydantic==2.10.6
docarray # ì¶”ê°€
```

## ìƒˆë¡œìš´ ì—ëŸ¬
```shell
AttributeError:Â st.session_state has no attribute "messages". Did you forget to initialize it? More info: https://docs.streamlit.io/develop/concepts/architecture/session-state#initialization

Traceback:

`   File "C:\dev\workspace\python-vibe-coding\langchain-chatbot\pages\4_ğŸ“„_chat_with_your_documents.py", line 111, in <module>     obj.main()     ~~~~~~~~^^  File "C:\dev\workspace\python-vibe-coding\langchain-chatbot\utils.py", line 35, in execute     func(*args, **kwargs)     ~~~~^^^^^^^^^^^^^^^^^  File "C:\dev\workspace\python-vibe-coding\langchain-chatbot\pages\4_ğŸ“„_chat_with_your_documents.py", line 89, in main     utils.display_msg(user_query, 'user')     ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^  File "C:\dev\workspace\python-vibe-coding\langchain-chatbot\utils.py", line 45, in display_msg     st.session_state.messages.append({"role": author, "content": msg})     ^^^^^^^^^^^^^^^^^^^^^^^^^  File "C:\Users\young\AppData\Roaming\Python\Python313\site-packages\streamlit\runtime\state\session_state_proxy.py", line 131, in __getattr__     raise AttributeError(_missing_attr_error_message(key))   `
```

![[Pasted image 20251126194927.png]]
ì´ ì˜¤ë¥˜ëŠ” **"ì±„íŒ… ë‚´ìš©ì„ ì €ì¥í•  ê³µê°„(`messages`)ì„ ì•„ì§ ë§Œë“¤ì§€ ì•Šì•˜ëŠ”ë°, ê±°ê¸°ì— ëŒ€í™”ë¥¼ ì €ì¥í•˜ë ¤ê³  í•´ì„œ"** ë°œìƒí•˜ëŠ” ë¬¸ì œì…ë‹ˆë‹¤.

Streamlitì€ í˜ì´ì§€ê°€ ìƒˆë¡œê³ ì¹¨ë  ë•Œë§ˆë‹¤ ì½”ë“œê°€ ë‹¤ì‹œ ì‹¤í–‰ë˜ëŠ”ë°, ì´ë•Œ ëŒ€í™” ê¸°ë¡ì„ ìœ ì§€í•˜ë ¤ë©´ `st.session_state`ë¼ëŠ” ì €ì¥ì†Œë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤. í˜„ì¬ ê·¸ ì €ì¥ì†Œ ì•ˆì— `messages`ë¼ëŠ” ë¦¬ìŠ¤íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ìƒíƒœì…ë‹ˆë‹¤.

ì½”ë“œë¥¼ í•œ ì¤„ë§Œ ì¶”ê°€í•˜ë©´ í•´ê²°
### ì½”ë“œ ìˆ˜ì •
**ìˆ˜ì •í•  ë¶€ë¶„**
```python
@utils.enable_chat_history
    def main(self):

        # [ì¶”ê°€í•  ì½”ë“œ] ëŒ€í™” ê¸°ë¡ ì €ì¥ì†Œ ì´ˆê¸°í™”
        if "messages" not in st.session_state:
            st.session_state["messages"] = []

        # User Inputs
        uploaded_files = st.sidebar.file_uploader(label='Upload PDF files', type=['pdf'], accept_multiple_files=True)
```

**ë®ì–´ì“°ê¸°ìš©**
```python
import os
import utils
import streamlit as st
from streaming import StreamHandler

from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import DocArrayInMemorySearch
from langchain_text_splitters import RecursiveCharacterTextSplitter


st.set_page_config(page_title="ChatPDF", page_icon="ğŸ“„")
st.header('Chat with your documents (Basic RAG)')
st.write('Has access to custom documents and can respond to user queries by referring to the content within those documents')
st.write('[![view source code ](https://img.shields.io/badge/view_source_code-gray?logo=github)](https://github.com/shashankdeshpande/langchain-chatbot/blob/master/pages/4_%F0%9F%93%84_chat_with_your_documents.py)')

class CustomDocChatbot:

    def __init__(self):
        utils.sync_st_session()
        self.llm = utils.configure_llm()
        self.embedding_model = utils.configure_embedding_model()

    def save_file(self, file):
        folder = 'tmp'
        if not os.path.exists(folder):
            os.makedirs(folder)
        
        file_path = f'./{folder}/{file.name}'
        with open(file_path, 'wb') as f:
            f.write(file.getvalue())
        return file_path

    @st.spinner('Analyzing documents..')
    def setup_qa_chain(self, uploaded_files):
        # Load documents
        docs = []
        for file in uploaded_files:
            file_path = self.save_file(file)
            loader = PyPDFLoader(file_path)
            docs.extend(loader.load())
        
        # Split documents and store in vector db
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        splits = text_splitter.split_documents(docs)
        vectordb = DocArrayInMemorySearch.from_documents(splits, self.embedding_model)

        # Define retriever
        retriever = vectordb.as_retriever(
            search_type='mmr',
            search_kwargs={'k':2, 'fetch_k':4}
        )

        # Setup memory for contextual conversation        
        memory = ConversationBufferMemory(
            memory_key='chat_history',
            output_key='answer',
            return_messages=True
        )

        # Setup LLM and QA chain
        qa_chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=retriever,
            memory=memory,
            return_source_documents=True,
            verbose=False
        )
        return qa_chain

    @utils.enable_chat_history
    def main(self):
        
        # [ìˆ˜ì •ëœ ë¶€ë¶„] ë©”ì‹œì§€ ì €ì¥ì†Œ ì´ˆê¸°í™” ì½”ë“œ ì¶”ê°€
        if "messages" not in st.session_state:
            st.session_state["messages"] = []

        # User Inputs
        uploaded_files = st.sidebar.file_uploader(label='Upload PDF files', type=['pdf'], accept_multiple_files=True)
        if not uploaded_files:
            st.error("Please upload PDF documents to continue!")
            st.stop()

        user_query = st.chat_input(placeholder="Ask me anything!")

        if uploaded_files and user_query:
            qa_chain = self.setup_qa_chain(uploaded_files)

            utils.display_msg(user_query, 'user')

            with st.chat_message("assistant"):
                st_cb = StreamHandler(st.empty())
                result = qa_chain.invoke(
                    {"question":user_query},
                    {"callbacks": [st_cb]}
                )
                response = result["answer"]
                st.session_state.messages.append({"role": "assistant", "content": response})
                utils.print_qa(CustomDocChatbot, user_query, response)

                # to show references
                for idx, doc in enumerate(result['source_documents'],1):
                    filename = os.path.basename(doc.metadata['source'])
                    page_num = doc.metadata['page']
                    ref_title = f":blue[Reference {idx}: *{filename} - page.{page_num}*]"
                    with st.popover(ref_title):
                        st.caption(doc.page_content)

if __name__ == "__main__":
    obj = CustomDocChatbot()
    obj.main()
```


# ìš”êµ¬ì‚¬í•­ : csv íŒŒì¼ì„ ì½ê³  ë¶„ì„í•˜ë„ë¡ ìˆ˜ì •
csv íŒŒì¼ë¡œ ì½ëŠ”ë‹¤ê³  í•˜ë‹ˆ ê¸°ëŠ¥ì„ ì¶”ê°€í•´ì•¼ë¨, aiì—ê²Œ ë§Œë“¤ì–´ ë‹¬ë¼ëŠ” í”„ë¡¬í”„íŠ¸ ë°›ì•„ì„œ ê²°ê³¼ê°’ ì €ì¥ì¥
## í”„ë¡¬í”„íŠ¸
ë„ˆëŠ” ì—„ì²­ë‚˜ê²Œ ë˜‘ë˜‘í•œ íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë¨¸ì•¼, ë‚´ê°€ ë§í•˜ëŠ” ì—…ë¬´ëŠ” í•  ìˆ˜ ìˆê² ì§€?
í˜„ì¬ íŒŒì¼ ê¸°ë°˜ ì±—ë´‡ ê¸°ëŠ¥ì„ ìˆ˜ì •í•´ì•¼ë¨. pdf ë¿ë§Œ ì•„ë‹ˆë¼ csvë„ ì§€ì›í•˜ë„ë¡ ìˆ˜ì •.
ì—…ë¡œë“œí•  csvì˜ ì¸ì½”ë”©ì€ utf-8ì„. ê¸°ëŠ¥ê³¼ ì´ì— ë§ê²Œ UI ë˜í•œ ìˆ˜ì •í•´ì¤˜.
ìœ í‹¸ ì½”ë“œë„ í˜¹ì‹œ ëª¨ë¥´ë‹ˆ ë³´ë‚´ì¤„ê²Œ, ë„ˆê°€ ë” ì¢‹ì€ ì½”ë“œë¥¼ ë§Œë“¤ê¸° ìœ„í•´ í•„ìš”í•œ ì½”ë“œê°€ ìˆë‹¤ë©´ ë§í•´ì¤˜.
ë‚´ê°€ ì˜ˆì‹œë¡œ ì‚¬ìš©í•  csvë„ ê°™ì´ ë³´ë‚´ì¤„ê²Œ.
### 4_ğŸ“„_chat_with_your_documents.py (í˜„ì¬ ì½”ë“œ)
```python
import os
import utils
import streamlit as st
from streaming import StreamHandler

from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import DocArrayInMemorySearch
from langchain_text_splitters import RecursiveCharacterTextSplitter


st.set_page_config(page_title="ChatPDF", page_icon="ğŸ“„")
st.header('Chat with your documents (Basic RAG)')
st.write('Has access to custom documents and can respond to user queries by referring to the content within those documents')
st.write('[![view source code ](https://img.shields.io/badge/view_source_code-gray?logo=github)](https://github.com/shashankdeshpande/langchain-chatbot/blob/master/pages/4_%F0%9F%93%84_chat_with_your_documents.py)')

class CustomDocChatbot:

    def __init__(self):
        utils.sync_st_session()
        self.llm = utils.configure_llm()
        self.embedding_model = utils.configure_embedding_model()

    def save_file(self, file):
        folder = 'tmp'
        if not os.path.exists(folder):
            os.makedirs(folder)
        
        file_path = f'./{folder}/{file.name}'
        with open(file_path, 'wb') as f:
            f.write(file.getvalue())
        return file_path

    @st.spinner('Analyzing documents..')
    def setup_qa_chain(self, uploaded_files):
        # Load documents
        docs = []
        for file in uploaded_files:
            file_path = self.save_file(file)
            loader = PyPDFLoader(file_path)
            docs.extend(loader.load())
        
        # Split documents and store in vector db
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        splits = text_splitter.split_documents(docs)
        vectordb = DocArrayInMemorySearch.from_documents(splits, self.embedding_model)

        # Define retriever
        retriever = vectordb.as_retriever(
            search_type='mmr',
            search_kwargs={'k':2, 'fetch_k':4}
        )

        # Setup memory for contextual conversation        
        memory = ConversationBufferMemory(
            memory_key='chat_history',
            output_key='answer',
            return_messages=True
        )

        # Setup LLM and QA chain
        qa_chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=retriever,
            memory=memory,
            return_source_documents=True,
            verbose=False
        )
        return qa_chain

    @utils.enable_chat_history
    def main(self):
        
        # [ìˆ˜ì •ëœ ë¶€ë¶„] ë©”ì‹œì§€ ì €ì¥ì†Œ ì´ˆê¸°í™” ì½”ë“œ ì¶”ê°€
        if "messages" not in st.session_state:
            st.session_state["messages"] = []

        # User Inputs
        uploaded_files = st.sidebar.file_uploader(label='Upload PDF files', type=['pdf'], accept_multiple_files=True)
        if not uploaded_files:
            st.error("Please upload PDF documents to continue!")
            st.stop()

        user_query = st.chat_input(placeholder="Ask me anything!")

        if uploaded_files and user_query:
            qa_chain = self.setup_qa_chain(uploaded_files)

            utils.display_msg(user_query, 'user')

            with st.chat_message("assistant"):
                st_cb = StreamHandler(st.empty())
                result = qa_chain.invoke(
                    {"question":user_query},
                    {"callbacks": [st_cb]}
                )
                response = result["answer"]
                st.session_state.messages.append({"role": "assistant", "content": response})
                utils.print_qa(CustomDocChatbot, user_query, response)

                # to show references
                for idx, doc in enumerate(result['source_documents'],1):
                    filename = os.path.basename(doc.metadata['source'])
                    page_num = doc.metadata['page']
                    ref_title = f":blue[Reference {idx}: *{filename} - page.{page_num}*]"
                    with st.popover(ref_title):
                        st.caption(doc.page_content)

if __name__ == "__main__":
    obj = CustomDocChatbot()
    obj.main()import os
import utils
import streamlit as st
from streaming import StreamHandler

from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import DocArrayInMemorySearch
from langchain_text_splitters import RecursiveCharacterTextSplitter


st.set_page_config(page_title="ChatPDF", page_icon="ğŸ“„")
st.header('Chat with your documents (Basic RAG)')
st.write('Has access to custom documents and can respond to user queries by referring to the content within those documents')
st.write('[![view source code ](https://img.shields.io/badge/view_source_code-gray?logo=github)](https://github.com/shashankdeshpande/langchain-chatbot/blob/master/pages/4_%F0%9F%93%84_chat_with_your_documents.py)')

class CustomDocChatbot:

    def __init__(self):
        utils.sync_st_session()
        self.llm = utils.configure_llm()
        self.embedding_model = utils.configure_embedding_model()

    def save_file(self, file):
        folder = 'tmp'
        if not os.path.exists(folder):
            os.makedirs(folder)
        
        file_path = f'./{folder}/{file.name}'
        with open(file_path, 'wb') as f:
            f.write(file.getvalue())
        return file_path

    @st.spinner('Analyzing documents..')
    def setup_qa_chain(self, uploaded_files):
        # Load documents
        docs = []
        for file in uploaded_files:
            file_path = self.save_file(file)
            loader = PyPDFLoader(file_path)
            docs.extend(loader.load())
        
        # Split documents and store in vector db
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        splits = text_splitter.split_documents(docs)
        vectordb = DocArrayInMemorySearch.from_documents(splits, self.embedding_model)

        # Define retriever
        retriever = vectordb.as_retriever(
            search_type='mmr',
            search_kwargs={'k':2, 'fetch_k':4}
        )

        # Setup memory for contextual conversation        
        memory = ConversationBufferMemory(
            memory_key='chat_history',
            output_key='answer',
            return_messages=True
        )

        # Setup LLM and QA chain
        qa_chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=retriever,
            memory=memory,
            return_source_documents=True,
            verbose=False
        )
        return qa_chain

    @utils.enable_chat_history
    def main(self):
        
        # [ìˆ˜ì •ëœ ë¶€ë¶„] ë©”ì‹œì§€ ì €ì¥ì†Œ ì´ˆê¸°í™” ì½”ë“œ ì¶”ê°€
        if "messages" not in st.session_state:
            st.session_state["messages"] = []

        # User Inputs
        uploaded_files = st.sidebar.file_uploader(label='Upload PDF files', type=['pdf'], accept_multiple_files=True)
        if not uploaded_files:
            st.error("Please upload PDF documents to continue!")
            st.stop()

        user_query = st.chat_input(placeholder="Ask me anything!")

        if uploaded_files and user_query:
            qa_chain = self.setup_qa_chain(uploaded_files)

            utils.display_msg(user_query, 'user')

            with st.chat_message("assistant"):
                st_cb = StreamHandler(st.empty())
                result = qa_chain.invoke(
                    {"question":user_query},
                    {"callbacks": [st_cb]}
                )
                response = result["answer"]
                st.session_state.messages.append({"role": "assistant", "content": response})
                utils.print_qa(CustomDocChatbot, user_query, response)

                # to show references
                for idx, doc in enumerate(result['source_documents'],1):
                    filename = os.path.basename(doc.metadata['source'])
                    page_num = doc.metadata['page']
                    ref_title = f":blue[Reference {idx}: *{filename} - page.{page_num}*]"
                    with st.popover(ref_title):
                        st.caption(doc.page_content)

if __name__ == "__main__":
    obj = CustomDocChatbot()
    obj.main()
```

### utils.py
```python
import os
import openai
import streamlit as st
from datetime import datetime
from streamlit.logger import get_logger
from langchain_openai import ChatOpenAI
from langchain_community.chat_models import ChatOllama
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings

logger = get_logger('Langchain-Chatbot')

#decorator
def enable_chat_history(func):
    if os.environ.get("OPENAI_API_KEY"):

        # to clear chat history after swtching chatbot
        current_page = func.__qualname__
        if "current_page" not in st.session_state:
            st.session_state["current_page"] = current_page
        if st.session_state["current_page"] != current_page:
            try:
                st.cache_resource.clear()
                del st.session_state["current_page"]
                del st.session_state["messages"]
            except:
                pass

        # to show chat history on ui
        if "messages" not in st.session_state:
            st.session_state["messages"] = [{"role": "assistant", "content": "How can I help you?"}]
        for msg in st.session_state["messages"]:
            st.chat_message(msg["role"]).write(msg["content"])

    def execute(*args, **kwargs):
        func(*args, **kwargs)
    return execute

def display_msg(msg, author):
    """Method to display message on the UI

    Args:
        msg (str): message to display
        author (str): author of the message -user/assistant
    """
    st.session_state.messages.append({"role": author, "content": msg})
    st.chat_message(author).write(msg)

def choose_custom_openai_key():
    openai_api_key = st.sidebar.text_input(
        label="OpenAI API Key",
        type="password",
        placeholder="sk-...",
        key="SELECTED_OPENAI_API_KEY"
        )
    if not openai_api_key:
        st.error("Please add your OpenAI API key to continue.")
        st.info("Obtain your key from this link: https://platform.openai.com/account/api-keys")
        st.stop()

    model = "gpt-4.1-mini"
    try:
        client = openai.OpenAI(api_key=openai_api_key)
        available_models = [{"id": i.id, "created":datetime.fromtimestamp(i.created)} for i in client.models.list() if str(i.id).startswith("gpt")]
        available_models = sorted(available_models, key=lambda x: x["created"])
        available_models = [i["id"] for i in available_models]

        model = st.sidebar.selectbox(
            label="Model",
            options=available_models,
            key="SELECTED_OPENAI_MODEL"
        )
    except openai.AuthenticationError as e:
        st.error(e.body["message"])
        st.stop()
    except Exception as e:
        print(e)
        st.error("Something went wrong. Please try again later.")
        st.stop()
    return model, openai_api_key

def configure_llm():
    available_llms = ["gpt-4.1-mini","llama3.2:3b","use your openai api key"]
    llm_opt = st.sidebar.radio(
        label="LLM",
        options=available_llms,
        key="SELECTED_LLM"
        )

    if llm_opt == "llama3.2:3b":
        llm = ChatOllama(model="llama3.2", base_url=st.secrets["OLLAMA_ENDPOINT"])
    elif llm_opt == "gpt-4.1-mini":
        llm = ChatOpenAI(model_name=llm_opt, temperature=0, streaming=True, api_key=st.secrets["OPENAI_API_KEY"])
    else:
        model, openai_api_key = choose_custom_openai_key()
        llm = ChatOpenAI(model_name=model, temperature=0, streaming=True, api_key=openai_api_key)
    return llm

def print_qa(cls, question, answer):
    log_str = "\nUsecase: {}\nQuestion: {}\nAnswer: {}\n" + "------"*10
    logger.info(log_str.format(cls.__name__, question, answer))

@st.cache_resource
def configure_embedding_model():
    embedding_model = FastEmbedEmbeddings(model_name="BAAI/bge-small-en-v1.5")
    return embedding_model

def sync_st_session():
    for k, v in st.session_state.items():
        st.session_state[k] = v

```

## ë³€ê²½ëœ ì½”ë“œ
```python
import os
import utils
import streamlit as st
from streaming import StreamHandler

from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain_community.document_loaders import PyPDFLoader, CSVLoader # CSVLoader ì¶”ê°€
from langchain_community.vectorstores import DocArrayInMemorySearch
from langchain_text_splitters import RecursiveCharacterTextSplitter


st.set_page_config(page_title="ChatPDF & CSV", page_icon="ğŸ“„") # ì œëª© ë³€ê²½
st.header('Chat with your documents (PDF & CSV)') # í—¤ë” ë³€ê²½
st.write('Has access to custom documents and can respond to user queries by referring to the content within those documents')
st.write('[![view source code ](https://img.shields.io/badge/view_source_code-gray?logo=github)](https://github.com/shashankdeshpande/langchain-chatbot/blob/master/pages/4_%F0%9F%93%84_chat_with_your_documents.py)')

class CustomDocChatbot:

    def __init__(self):
        utils.sync_st_session()
        self.llm = utils.configure_llm()
        self.embedding_model = utils.configure_embedding_model()

    def save_file(self, file):
        folder = 'tmp'
        if not os.path.exists(folder):
            os.makedirs(folder)
        
        file_path = f'./{folder}/{file.name}'
        with open(file_path, 'wb') as f:
            f.write(file.getvalue())
        return file_path

    @st.spinner('Analyzing documents..')
    def setup_qa_chain(self, uploaded_files):
        # Load documents
        docs = []
        for file in uploaded_files:
            file_path = self.save_file(file)
            file_ext = os.path.splitext(file_path)[1].lower() # í™•ì¥ì ì¶”ì¶œ
            
            # í™•ì¥ìì— ë”°ë¥¸ ë¡œë” ë¶„ê¸° ì²˜ë¦¬
            if file_ext == '.pdf':
                loader = PyPDFLoader(file_path)
                docs.extend(loader.load())
            elif file_ext == '.csv':
                # ìš”ì²­í•˜ì‹ ëŒ€ë¡œ utf-8 ì¸ì½”ë”© ì„¤ì •
                loader = CSVLoader(file_path, encoding='utf-8')
                docs.extend(loader.load())
            else:
                st.warning(f"Unsupported file type: {file.name}")
                continue
        
        # Split documents and store in vector db
        # CSVì˜ ê²½ìš° ê° í–‰ì´ í•˜ë‚˜ì˜ ë¬¸ì„œê°€ ë˜ì§€ë§Œ, ë°ì´í„°ê°€ ê¸¸ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ìŠ¤í”Œë¦¬í„°ëŠ” ìœ ì§€í•©ë‹ˆë‹¤.
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        splits = text_splitter.split_documents(docs)
        vectordb = DocArrayInMemorySearch.from_documents(splits, self.embedding_model)

        # Define retriever
        retriever = vectordb.as_retriever(
            search_type='mmr',
            search_kwargs={'k':2, 'fetch_k':4}
        )

        # Setup memory for contextual conversation        
        memory = ConversationBufferMemory(
            memory_key='chat_history',
            output_key='answer',
            return_messages=True
        )

        # Setup LLM and QA chain
        qa_chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=retriever,
            memory=memory,
            return_source_documents=True,
            verbose=False
        )
        return qa_chain

    @utils.enable_chat_history
    def main(self):
        
        # ë©”ì‹œì§€ ì €ì¥ì†Œ ì´ˆê¸°í™”
        if "messages" not in st.session_state:
            st.session_state["messages"] = []

        # User Inputs
        # typeì— 'csv' ì¶”ê°€ ë° ë¼ë²¨ ìˆ˜ì •
        uploaded_files = st.sidebar.file_uploader(
            label='Upload PDF or CSV files', 
            type=['pdf', 'csv'], 
            accept_multiple_files=True
        )
        
        if not uploaded_files:
            st.error("Please upload PDF or CSV documents to continue!")
            st.stop()

        user_query = st.chat_input(placeholder="Ask me anything!")

        if uploaded_files and user_query:
            qa_chain = self.setup_qa_chain(uploaded_files)

            utils.display_msg(user_query, 'user')

            with st.chat_message("assistant"):
                st_cb = StreamHandler(st.empty())
                result = qa_chain.invoke(
                    {"question":user_query},
                    {"callbacks": [st_cb]}
                )
                response = result["answer"]
                st.session_state.messages.append({"role": "assistant", "content": response})
                utils.print_qa(CustomDocChatbot, user_query, response)

                # to show references
                for idx, doc in enumerate(result['source_documents'],1):
                    filename = os.path.basename(doc.metadata['source'])
                    
                    # PDFëŠ” page ë©”íƒ€ë°ì´í„°ê°€ ìˆì§€ë§Œ CSVëŠ” row(í–‰) ë²ˆí˜¸ê°€ ìˆìŠµë‹ˆë‹¤. 
                    # CSVLoaderëŠ” ê¸°ë³¸ì ìœ¼ë¡œ 'row' ë©”íƒ€ë°ì´í„°ë¥¼ ì¤ë‹ˆë‹¤. ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ì²˜ë¦¬í•©ë‹ˆë‹¤.
                    if 'page' in doc.metadata:
                        loc_info = f"page.{doc.metadata['page']}"
                    elif 'row' in doc.metadata:
                        loc_info = f"row.{doc.metadata['row']}"
                    else:
                        loc_info = "unknown location"

                    ref_title = f":blue[Reference {idx}: *{filename} - {loc_info}*]"
                    with st.popover(ref_title):
                        st.caption(doc.page_content)

if __name__ == "__main__":
    obj = CustomDocChatbot()
    obj.main()
```

# í…ŒìŠ¤íŠ¸
## ì‘ë‹µ í™•ì¸ í…ŒìŠ¤íŠ¸

![[Pasted image 20251126201946.png]]
ë©‹ëŒ€ë¡œ ì•„ëŠ”ì²™ í•˜ëŠ”ê²Œ ì•„ë‹ˆë¼ í•  ìˆ˜ ì—†ë‹¤ë©´ í•  ìˆ˜ ì—†ë‹¤ê³  ì†”ì§í•˜ê²Œ ë§í•˜ëŠ”ê±¸ ì•Œ ìˆ˜ ìˆìŒ

![[Pasted image 20251126202206.png]]
ì¼ë‹¨ csv ë°ì´í„° ìì²´ëŠ” ì˜ ì½ëŠ”ê±° ê°™ìŒ.

ê·¸ë ‡ë‹¤ë©´ ë” ìì„¸íˆ ë¬¼ì–´ë³¼ê¹Œ?
![[Pasted image 20251126202549.png]]


![[Pasted image 20251126203254.png]]

ì´ë ‡ê²Œ ëŒ€ë‹µí•˜ê³  ë‚˜ì˜¨ ëˆì€ 0.03ë‹¬ëŸ¬ì„

# ë§ˆë¬´ë¦¬ ìµœì¢… ì‘ì—…
ì´ì œ í•œê¸€í™”ë‘ ê°–ë‹¤ ë² ë‚€ê±°ì§€ë§Œ í‹°ê°€ ì•ˆë‚˜ë„ë¡ íƒˆë°”ê¿ˆ í• ê±°ì„.

---

ë„ˆëŠ” ì •ë§ë¡œ ë˜‘ë˜‘í•œ íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë¨¸ì•¼, í”„ë¡œì íŠ¸ë¥¼ íŒŒì•…í•˜ê³  ìš”êµ¬ì‚¬í•­ì„ ì¤€ìˆ˜í•˜ì—¬ í”„ë¡œì íŠ¸ ëª©í‘œë¥¼ ë‹¬ì„±í•´ì¤˜.
## í”„ë¡œì íŠ¸ ëª©ì 
ë” ë˜‘ë˜‘í•œ ì±—ë´‡ ë§Œë“¤ê¸°
## ìš”êµ¬ì‚¬í•­
ê¸°ì¡´ì— ê¹ƒí—ˆë¸Œì— ìˆëŠ” ë ˆí¬ì§€í† ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‚´ê°€ ì›í•˜ëŠ” í”„ë¡œì íŠ¸ë¡œ ì‚´ì§ ìˆ˜ì •í–ˆìŒ.
í˜„ì¬ ë‚´ê°€ 1ì°¨ì ìœ¼ë¡œ íŒŒì¼ ì—…ë¡œë“œ ê¸°ë°˜ ì±—ë´‡ì„ ë§Œë“¤ì—ˆì–´ (`4_ğŸ“„_chat_with_your_documents.py`)
ì¢€ ë” ë˜‘ë˜‘í•œ ì±—ë´‡ìœ¼ë¡œ ë§Œë“¤ê³  ì‹¶ì–´.
ì§€ê¸ˆ í˜„ì¬ í˜ì´ì§€ êµ¬ì„±ì´ì•¼, ë‚´ê°€ ì¶”ê°€ë¡œ ì‘ì—…í•œ í˜ì´ì§€ì¸ ë¬¸ì„œì™€ ì±„íŒ…í•˜ê¸°ë¥¼ ë©‹ì§€ê²Œ ì—…ê·¸ë ˆì´ë“œ í•˜ê³  ì‹¶ì–´.
ë¬¸ì„œì™€ ì±„íŒ…í•˜ê¸° ì±—ë´‡ì„ ìƒí™© ì¸ì‹ì„ í•˜ë©´ì„œë„ ìµœê·¼ ì´ë²¤íŠ¸ì— ëŒ€í•´ ëŒ€í™”ê°€ ê°€ëŠ¥í•˜ë„ë¡ ì¸í„°ë„· ê¸°ëŠ¥ì„ ì¶”ê°€í•  ìˆ˜ ìˆì„ê¹Œ? ì¦‰, 4ë²ˆ í˜ì´ì§€ì— 2ë²ˆê³¼ 3ë²ˆ ê¸°ëŠ¥ì„ ì¶”ê°€í•˜ëŠ”ê±°ì§€!
í•  ìˆ˜ ìˆë‹¤ë©´ 4ë²ˆ í˜ì´ì§€(`4_ğŸ“„_chat_with_your_documents.py`)ì— ê¸°ëŠ¥ì„ ì¶”ê°€í•´ì¤˜.
### í˜„ì¬ í˜ì´ì§€
- **1. ê¸°ë³¸ ì±—ë´‡**  
    LLMê³¼ ëŒ€í™”í˜• ëŒ€í™”ë¥¼ ë‚˜ëˆ ë³´ì„¸ìš”.
- **2. ìƒí™© ì¸ì‹ ì±—ë´‡**  
    ì´ì „ ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ê³  ê·¸ì— ë”°ë¼ ì‘ë‹µì„ ì œê³µí•˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤.
- **3. ì¸í„°ë„· ì ‘ì†ì´ ê°€ëŠ¥í•œ ì±—ë´‡**  
    ìµœê·¼ ì´ë²¤íŠ¸ì— ëŒ€í•œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µí•  ìˆ˜ ìˆëŠ” ì¸í„°ë„· ì§€ì› ì±—ë´‡ì…ë‹ˆë‹¤.
- **4. ë¬¸ì„œì™€ ì±„íŒ…í•˜ê¸° (ë‚´ê°€ ì¶”ê°€ë¡œ ì‘ì—…í•œ í˜ì´ì§€)**  
    ì±—ë´‡ì´ ì‚¬ìš©ì ì •ì˜ ë¬¸ì„œì— ì•¡ì„¸ìŠ¤í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì„ ì œê³µí•˜ì—¬ ì°¸ì¡°ëœ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆì˜ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
- **5. SQL ë°ì´í„°ë² ì´ìŠ¤ì™€ ì±„íŒ…í•˜ê¸°**  
    ê°„ë‹¨í•œ ëŒ€í™”í˜• ëª…ë ¹ì„ í†µí•´ ì±—ë´‡ì´ SQL ë°ì´í„°ë² ì´ìŠ¤ì™€ ìƒí˜¸ ì‘ìš©í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
- **6. ì›¹ì‚¬ì´íŠ¸ì™€ ì±„íŒ…**  
    ì±—ë´‡ì´ ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸ ì™€ ìƒí˜¸ ì‘ìš©í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
## ì£¼ì˜ì‚¬í•­
- ì¶”ê°€ì ì¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ì„¤ì¹˜í•˜ì§€ ì•ŠëŠ”ë‹¤.
- ë„ˆë¬´ ë§ì€ ê¸°ëŠ¥ì„ ë§Œë“¤ì§€ëŠ” ì•ŠëŠ”ë‹¤.
### í˜„ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¦¬ìŠ¤íŠ¸
```
Package                   Version
------------------------- ---------------
aiohappyeyeballs          2.6.1
aiohttp                   3.13.2
aiosignal                 1.4.0
altair                    5.5.0
annotated-types           0.7.0
anyio                     4.11.0
attrs                     25.4.0
blinker                   1.9.0
cachetools                5.5.2
certifi                   2025.11.12
charset-normalizer        3.4.4
click                     8.3.1
colorama                  0.4.6
coloredlogs               15.0.1
dataclasses-json          0.6.7
distro                    1.9.0
docarray                  0.41.0
duckduckgo_search         7.0.1
fastembed                 0.7.3
filelock                  3.20.0
flatbuffers               25.9.23
frozenlist                1.8.0
fsspec                    2025.10.0
gitdb                     4.0.12
GitPython                 3.1.45
greenlet                  3.2.4
h11                       0.16.0
httpcore                  1.0.9
httpx                     0.28.1
httpx-sse                 0.4.3
huggingface-hub           0.36.0
humanfriendly             10.0
idna                      3.11
Jinja2                    3.1.6
jiter                     0.12.0
jsonpatch                 1.33
jsonpointer               3.0.0
jsonschema                4.25.1
jsonschema-specifications 2025.9.1
langchain                 0.3.13
langchain-community       0.3.13
langchain-core            0.3.28
langchain-openai          0.2.14
langchain-text-splitters  0.3.4
langchainhub              0.1.21
langgraph                 1.0.4
langgraph-checkpoint      3.0.1
langgraph-prebuilt        1.0.5
langgraph-sdk             0.2.10
langsmith                 0.2.11
loguru                    0.7.3
lxml                      6.0.2
markdown-it-py            4.0.0
MarkupSafe                3.0.3
marshmallow               3.26.1
mdurl                     0.1.2
mmh3                      5.2.0
mpmath                    1.3.0
multidict                 6.7.0
mypy_extensions           1.1.0
narwhals                  2.12.0
numpy                     2.3.5
onnxruntime               1.23.2
openai                    1.58.1
orjson                    3.11.4
ormsgpack                 1.12.0
packaging                 23.2
pandas                    2.3.3
pillow                    11.3.0
pip                       25.2
primp                     0.15.0
propcache                 0.4.1
protobuf                  5.29.5
py_rust_stemmers          0.1.5
pyarrow                   21.0.0
pydantic                  2.10.6
pydantic_core             2.27.2
pydantic-settings         2.12.0
pydeck                    0.9.1
Pygments                  2.19.2
pypdf                     5.1.0
pyreadline3               3.5.4
python-dateutil           2.9.0.post0
python-dotenv             1.2.1
pytz                      2025.2
PyYAML                    6.0.3
referencing               0.37.0
regex                     2025.11.3
requests                  2.32.3
requests-toolbelt         1.0.0
rich                      13.9.4
rpds-py                   0.29.0
six                       1.17.0
smmap                     5.0.2
sniffio                   1.3.1
SQLAlchemy                2.0.36
streamlit                 1.41.1
sympy                     1.14.0
tenacity                  8.5.0
tiktoken                  0.12.0
tokenizers                0.22.1
toml                      0.10.2
tornado                   6.5.2
tqdm                      4.67.1
types-requests            2.32.4.20250913
typing_extensions         4.15.0
typing-inspect            0.9.0
typing-inspection         0.4.2
tzdata                    2025.2
urllib3                   2.5.0
validators                0.34.0
watchdog                  6.0.0
win32_setctime            1.2.0
xxhash                    3.6.0
yarl                      1.22.0
zstandard                 0.25.0
```
### í˜„ì¬ íŒŒì´ì¬ ë²„ì „
```shell
C:\Users\young>python
Python 3.13.7 (tags/v3.13.7:bcee1c3, Aug 14 2025, 14:15:11) [MSC v.1944 64 bit (AMD64)] on win32
Type "help", "copyright", "credits" or "license" for more information.
```

---
## ê²°ê³¼ë¬¼
```python
import os
import utils
import streamlit as st
from streaming import StreamHandler

from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain_community.document_loaders import PyPDFLoader, CSVLoader # CSVLoader ì¶”ê°€
from langchain_community.vectorstores import DocArrayInMemorySearch
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain import hub
from langchain_community.tools import DuckDuckGoSearchRun
from langchain.agents import AgentExecutor, create_react_agent
from langchain_core.tools import Tool


st.set_page_config(page_title="Smart Document Chatbot", page_icon="ğŸ“„") # ì œëª© ë³€ê²½
st.header('Smart Document Chatbot with Web Access') # í—¤ë” ë³€ê²½
st.write('Enhanced chatbot with context awareness, document access, and internet search capabilities')
st.write('[![view source code ](https://img.shields.io/badge/view_source_code-gray?logo=github)](https://github.com/shashankdeshpande/langchain-chatbot/blob/master/pages/4_%F0%9F%93%84_chat_with_your_documents.py)')

class SmartDocChatbot:

    def __init__(self):
        utils.sync_st_session()
        self.llm = utils.configure_llm()
        self.embedding_model = utils.configure_embedding_model()
        self.qa_chain = None
        self.memory = None

    def save_file(self, file):
        folder = 'tmp'
        if not os.path.exists(folder):
            os.makedirs(folder)
        
        file_path = f'./{folder}/{file.name}'
        with open(file_path, 'wb') as f:
            f.write(file.getvalue())
        return file_path

    @st.spinner('Setting up retrieval chain with web search capability..')
    def setup_qa_chain_with_search(self, uploaded_files):
        """
        Setup QA chain with document retrieval and web search capabilities
        """
        # Load documents
        docs = []
        for file in uploaded_files:
            file_path = self.save_file(file)
            file_ext = os.path.splitext(file_path)[1].lower() # í™•ì¥ì ì¶”ì¶œ
            
            # í™•ì¥ìì— ë”°ë¥¸ ë¡œë” ë¶„ê¸° ì²˜ë¦¬
            if file_ext == '.pdf':
                loader = PyPDFLoader(file_path)
                docs.extend(loader.load())
            elif file_ext == '.csv':
                # ìš”ì²­í•˜ì‹ ëŒ€ë¡œ utf-8 ì¸ì½”ë”© ì„¤ì •
                loader = CSVLoader(file_path, encoding='utf-8')
                docs.extend(loader.load())
            else:
                st.warning(f"Unsupported file type: {file.name}")
                continue
        
        # Split documents and store in vector db
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        splits = text_splitter.split_documents(docs)
        vectordb = DocArrayInMemorySearch.from_documents(splits, self.embedding_model)

        # Define retriever
        retriever = vectordb.as_retriever(
            search_type='mmr',
            search_kwargs={'k':2, 'fetch_k':4}
        )

        # Setup memory for contextual conversation        
        memory = ConversationBufferMemory(
            memory_key='chat_history',
            output_key='answer',
            return_messages=True
        )

        # Setup web search tool
        ddg_search = DuckDuckGoSearchRun()
        web_search_tool = Tool(
            name="WebSearch",
            func=ddg_search.run,
            description="Useful for searching recent information on the internet when documents don't contain the answer"
        )

        # Setup LLM and QA chain with document retrieval
        qa_chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=retriever,
            memory=memory,
            return_source_documents=True,
            verbose=False
        )
        
        return qa_chain, memory, web_search_tool

    def query_web_for_supplement(self, question, chat_history):
        """
        Use web search to supplement document-based answers
        """
        try:
            # Simple web search using DuckDuckGo
            search_tool = DuckDuckGoSearchRun()
            search_results = search_tool.run(question)
            return search_results
        except Exception as e:
            st.warning(f"Web search error: {str(e)}")
            return None

    @utils.enable_chat_history
    def main(self):
        
        # ë©”ì‹œì§€ ì €ì¥ì†Œ ì´ˆê¸°í™”
        if "messages" not in st.session_state:
            st.session_state["messages"] = []

        # User Inputs
        # typeì— 'csv' ì¶”ê°€ ë° ë¼ë²¨ ìˆ˜ì •
        uploaded_files = st.sidebar.file_uploader(
            label='Upload PDF or CSV files', 
            type=['pdf', 'csv'], 
            accept_multiple_files=True
        )
        
        # Add option to enable web search
        enable_web_search = st.sidebar.checkbox(
            "Enable Web Search for Recent Information",
            value=True,
            help="Search the web to supplement answers when documents don't have enough info"
        )
        
        if not uploaded_files:
            st.error("Please upload PDF or CSV documents to continue!")
            st.stop()

        user_query = st.chat_input(placeholder="Ask me anything!")

        if uploaded_files and user_query:
            qa_chain, memory, web_search_tool = self.setup_qa_chain_with_search(uploaded_files)

            utils.display_msg(user_query, 'user')

            with st.chat_message("assistant"):
                st_cb = StreamHandler(st.empty())
                result = qa_chain.invoke(
                    {"question":user_query},
                    {"callbacks": [st_cb]}
                )
                response = result["answer"]
                
                # If web search is enabled, supplement with web search results
                if enable_web_search:
                    # Check if answer might benefit from web search (for recent topics)
                    keywords = ["recent", "latest", "2024", "2025", "today", "news", "current"]
                    if any(keyword.lower() in user_query.lower() for keyword in keywords):
                        web_results = self.query_web_for_supplement(user_query, memory.chat_memory.messages)
                        if web_results:
                            response += f"\n\n**ğŸ“° Latest Information from Web Search:**\n{web_results}"
                
                st.session_state.messages.append({"role": "assistant", "content": response})
                utils.print_qa(SmartDocChatbot, user_query, response)

                # to show references
                st.markdown("---")
                st.markdown("**ğŸ“š Document References:**")
                for idx, doc in enumerate(result['source_documents'],1):
                    filename = os.path.basename(doc.metadata['source'])
                    
                    # PDFëŠ” page ë©”íƒ€ë°ì´í„°ê°€ ìˆì§€ë§Œ CSVëŠ” row(í–‰) ë²ˆí˜¸ê°€ ìˆìŠµë‹ˆë‹¤. 
                    # CSVLoaderëŠ” ê¸°ë³¸ì ìœ¼ë¡œ 'row' ë©”íƒ€ë°ì´í„°ë¥¼ ì¤ë‹ˆë‹¤. ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ì²˜ë¦¬í•©ë‹ˆë‹¤.
                    if 'page' in doc.metadata:
                        loc_info = f"page.{doc.metadata['page']}"
                    elif 'row' in doc.metadata:
                        loc_info = f"row.{doc.metadata['row']}"
                    else:
                        loc_info = "unknown location"

                    ref_title = f":blue[Reference {idx}: *{filename} - {loc_info}*]"
                    with st.popover(ref_title):
                        st.caption(doc.page_content)

if __name__ == "__main__":
    obj = SmartDocChatbot()
    obj.main()
```

## í† í° ì‚¬ìš©ëŸ‰
5402 -> 


![[Pasted image 20251126211803.png]]

![[Pasted image 20251126211911.png]]